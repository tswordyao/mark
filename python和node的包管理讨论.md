```

> pip install 是把包全都装在了全局
用 virtualenv，python 的环境可以非常干净的(每个项目可以创建在python虚拟环境里,避免中心库中依赖包版本不同困扰)
不会玩 Virtualenv 么? 直接把整个环境打包移到目标环境就行了. 最笨的办法 pip download 包在本地目录.

> 打开用户目录，一大堆以点开头的文件夹
你说的点开头的文件是什么？就算装到系统里面，也是在 site-package 里面


pip 是不考虑安装环境的，如果你要安装的库里面需要系统环境支持的话，就需要自己把环境配好。
比如 lxmlx，就是个大魔王，网上搜下，多少人躺过这个坑。
如果不涉及 C 库或者系统环境还好，不然还真的蛮头疼的。
所以 conda 就是为了解决这个的，真的很好用，安装包的时候，顺带帮你把这个包所需的环境包也装了。


带 C 拓展的确实很复杂，尤其是 Linux，不同系统依赖的动态链接库不一致，在脚本语言里是硬伤，npm 也有类似的问题，只不过它很少带 C 拓展的库。docker 按理说不会有这种问题。


是因为 python 不像 js 包分的很细很多，py 项目总的包数量是很少的，所以还能处理。如果 py 也像 js 几行代码都能做一个包，那他这个设计早就崩溃了。
(反过来说, js的标准库缺失导致小包太多了. 所以也不得不放弃中心库设计.)


用了 golang 的包管理 你就会觉得 pip 是多么优秀了


npm 能嘲笑的大概只有 go 和 C++了吧？


python10 年前的代码，今天还能用。
nodejs10 个月前的代码，今天怎么用？


一般教程不都是教你先安装一个虚拟环境的么 这样可以避免包的混乱和版本冲突


其实大多数包，在 archlinux 下用 pacman 或者 AUR 装，都自动处理好了 C 库的依赖。
装 python-lxml 的时候，会自动安装依赖 libxslt，装 python-scipy 会自动安装 scipy
关于包的列表，以行来分割有很大的方便性。sed、grep、xarg、cut 等等的工具都是以行为单位处理的。
我就是通过 pip freeze > pip_package_list 来备份包列表，同时把列表放在 dotfiles 里面的。
我最近尝试在我的 dotfiles 下加一个 Dockerfile，通过 docker 来让我的服务器保持跟我桌面一样的环境（装相同的软件，相同的 zsh、vim 配置）。
我在 Dockerfile 里的其中一个操作就是 pip install -r <(cut -d = -f 1 pip_package_list) --user，通过 cut 命令来把版本号截掉，从而让 pip 安装最新版本。


> 其中提到很有意思的一个事情，也就是 python 不允许一个 library 存在两个版本。但假如说你有两个包，包 A 依赖包 C@1.1，包 B 依赖包 C@1.2。你用 pip 装完 A 和 B，你会发现包 C 是版本 1.1 *或者* 1.2 （看你安装的顺序）。
发生这种事情本来就是不科学的，不过我还没遇到过。好像 pipenv 在解决这个问题


如果一个项目依赖了两个包,  而这两个包分别依赖一个包X的不同版本,  由于python 不允许一个包有多个版本同时存在。那么必定有一个包的引用X的版本会被修改为与另一引用相同, 这可能导致问题.
这样，你的程序没有重现的绝对保障。换句话来说，你的程序能不能跑有些看运气。


知道你说的这种情况，但是真的我自己没遇到过…… 一般的包都会努力去兼容最新的依赖。
像你说的这个例子，我觉得根源不在包管理器上，而在设计的软件向后兼容上。这个问题不像是包管理器能解决的吧。就像 a.py 用的 python2 ； b.py 用的 python3，想要在一个程序 import a, b 怎么可能呢？


您很少遇到不代表别人很少遇到，我 debug 这种依赖相关的 bug 应该不算少，总是要 pip uninstall，install，然后通过一番努力终于代码能跑。然而我厌倦了这种 debug，既然有 npm 这种一键解决你依赖问题的安装方式，为什么每次要自己找麻烦去手动解决依赖 bug 呢？


还有反驳说: 你太依赖“每个依赖包版本必须精确到 minor version ”这个事实了。Python 社区的 major version 兼容性和 depreciate api 是好事，JS 那种根本不敢升级依赖包版本的才是耍流氓。开源软件包修个小 bug 是常见的事情，Python 升级依赖包版本我基本都是无脑做的，npm 你升级依赖包版本哪次不是胆战心惊。


Python 社区有很多“约定俗成”，你得习惯它们，并且享受它们。譬如 x.y.z，社区标准是保证 x. 不变程序就能跑，y 不变不会加入新功能。大家写程序也会考虑这个原则，如果不能，那么这个包大概是无法成为知名的包。
当然，特殊规定是，0.y.z 不考虑兼容性。直到 1.0.0 以后才有这个规定。所以你举得例子 pandas，不巧它才 0.20 。NumPy 已经 1.0 了，TensorFlow 也是，所以可以期待其兼容性。



我第一次写的小 /中型项目 https://github.com/streettraffic/streettraffic 用到了 websockets==3.3 这个依赖。我 setup.py 是乱写的，连版本号都没写，所以我写完几个月重装时发现跑不了了，后来 debug 之后发现是 websockets 高等版本破坏了原有的 api。

哪怕我在 install_requires 里面写了 websockets==3.3，只要别人在安装我的项目之后再安装一个项目有用到 websockets 的更高版本，我的代码就会报错了。

是的，我可以维护这个项目去修 bug，但是每个人的精力是有限的，并且你主要工作的项目也会变，所以我并不愿意改这个 api。我并不认为这样是错误的，并且我认为我写的代码如果能跑，便不应该因为这种依赖问题到后期某一天报错。



All right，你说的问题确实存在，但是 virtual env / anaconda env / docker 不是都能解决你的问题嘛？我觉得总有一款合适，所以不是什么大问题。
为什么不把 pip + virtual env / anaconda env 当做一个整体看，那样不就清静了么？还有 @kslr 您说的这个需求，Anaconda 就支持。如果你更勤快点，Docker 也不错。总而言之您二位为什么就偏要把 pip 单独拿出来批判一番呢？


conda 新建 environment 也很简单啊，多版本管理没什么问题的


其实我也主要用 Python，不过可能我写的程序不是很业务，而且引入依赖也比较谨慎（小库基本上 copy-paste-modify-test，好处是自己的单元测试比较放心），所以没啥大问题。


requirements.txt 还是只在开发中使用比较好，有分发需求的话依赖最好写 setup.py 里；
pip 确实存在一些问题，比如处理依赖关系上，安装可能会需要手动使用系统包管理安装一些东西，卸载不能自动清理已经不被依赖的包（也许可以但我不知道）。


npm 的项目，放半年之后 npm update 一次保证跑不起来。


我常用 pyenv，尝试了一下 pipenv，确实很炫酷，但是感觉很不顺手…
用 pipenv 的话 virtualenv 肯定要全交给 pipenv 管理，可是 pipenv 各种操作都是直接无视 pyenv，比如 pipenv install --system ***把 package 装到了我也不知道在哪的地方
而且自带 flake 等等让我觉得 pipenv 有点做的太多了
如果能有一款只包含 pipfile[.lock]管理的工具就好了，其他方面我觉得还是 pyenv 好用
综上，滚回 pyenv 了:)
如果有正确的 pyenv+pipenv 使用方式，请务必告诉我


部署我不觉得要打成 rpm，rpm 只保证兼容 red hat 系。我觉得在 Dockerfile 里写 pip 打包是 docker 镜像才是最佳实践

```